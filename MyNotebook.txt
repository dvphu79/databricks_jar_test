import org.apache.spark.sql._
import org.apache.spark.sql.types._


  private val serviceCredential = dbutils.secrets.get(scope = "key-vault-secret", key = "service-credential-secret")
  private val storageAccountAccessKey = dbutils.secrets.get(scope = "key-vault-secret", key = "storage-account-access-key-secret")

  private val appId = "254987db-5fcc-4025-94a2-6cbe94b75c5c"
  private val tenantId = "e85413be-9893-4b17-ac77-83c4443a22a3"

  private val username = "sqladminuser@synapse79"
  private val password = dbutils.secrets.get(scope = "key-vault-secret", key = "sql-admin-password-secret")
  private val server = "synapse79.sql.azuresynapse.net:1433"
  private val database = "SQLPOOL1"

  private val containerName = "level2"
  private val storageAccountName = "storageaccount779"

  private val fileName1 = "userdata1.parquet"
  private val fileName2 = "userdata2.parquet"
  private val fileName3 = "userdata3.parquet"
  private val fileName4 = "userdata4.parquet"
  private val fileName5 = "userdata5.parquet"


  //

  println("START MY JAR")

  //


  println("CONFIG SPARK")
  println("Connect to Azure Data Lake Storage Gen2 or Blob Storage using OAuth 2.0 with an Azure service principal")

  spark.conf.set(s"fs.azure.account.auth.type.$storageAccountName.dfs.core.windows.net", "OAuth")
  spark.conf.set(s"fs.azure.account.oauth.provider.type.$storageAccountName.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
  spark.conf.set(s"fs.azure.account.oauth2.client.id.$storageAccountName.dfs.core.windows.net", appId)
  spark.conf.set(s"fs.azure.account.oauth2.client.secret.$storageAccountName.dfs.core.windows.net", serviceCredential)
  spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.$storageAccountName.dfs.core.windows.net", s"https://login.microsoftonline.com/$tenantId/oauth2/token")


  //

  println("START MOUNT")
  println("Config using OAuth 2.0 with an Azure service principal")

  val configs = Map(
    "fs.azure.account.auth.type" -> "OAuth",
    "fs.azure.account.oauth.provider.type" -> "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
    "fs.azure.account.oauth2.client.id" -> appId,
    "fs.azure.account.oauth2.client.secret" -> serviceCredential,
    "fs.azure.account.oauth2.client.endpoint" -> s"https://login.microsoftonline.com/$tenantId/oauth2/token",
    "fs.azure.createRemoteFileSystemDuringInitialization" -> "true")

  try {
    dbutils.fs.unmount(s"/mnt/$containerName")
    println(s"success unmounted /mnt/$containerName")
  } catch {
    case exception: Exception => println(s"failed unmounted /mnt/$containerName")
  }

  dbutils.fs.mount(
    source = s"abfss://$containerName@$storageAccountName.dfs.core.windows.net",
    mountPoint = s"/mnt/$containerName",
    extraConfigs = configs)
  println(s"success mounted /mnt/$containerName")
  println(dbutils.fs.ls(s"/mnt/$containerName"))


  //

  println("READ DATA FROM FILES")

  val mydf1 = readDataFromContainer(spark, fileName1)
  val mydf2 = readDataFromContainer(spark, fileName2)
  val mydf3 = readDataFromContainer(spark, fileName3)
  val mydf4 = readDataFromContainer(spark, fileName4)
  val mydf5 = readDataFromContainer(spark, fileName5)

  //

  println("CREATE EXPECTED SCHEMA")

  // The schema is encoded in a string
  var schemaString = "first_name last_name email gender ip_address cc country birthdate title comments"
  // Generate the schema based on the string of schema
  val fieldsString = schemaString.split(" ")
    .map(fieldName => StructField(fieldName, StringType, nullable = true))
  schemaString = "id"
  val fieldsInteger = schemaString.split(" ")
    .map(fieldName => StructField(fieldName, IntegerType, nullable = true))
  schemaString = "salary"
  val fieldsDouble = schemaString.split(" ")
    .map(fieldName => StructField(fieldName, DoubleType, nullable = true))
  schemaString = "registration_dttm"
  val fieldsTimestamp = schemaString.split(" ")
    .map(fieldName => StructField(fieldName, TimestampType, nullable = true))
  val fields = fieldsString ++ fieldsInteger ++ fieldsDouble ++ fieldsTimestamp
  val expectedSchema = StructType(fields)
  expectedSchema.printTreeString()

  //

  println("CHECK VALIDATION ON DATA FILES AND FILTER VALID DATA FRAMES")

  val validCheck1 = checkValidationOnDataFiles(mydf1, fileName1, expectedSchema)
  val validCheck2 = checkValidationOnDataFiles(mydf2, fileName2, expectedSchema)
  val validCheck3 = checkValidationOnDataFiles(mydf3, fileName3, expectedSchema)
  val validCheck4 = checkValidationOnDataFiles(mydf4, fileName4, expectedSchema)
  val validCheck5 = checkValidationOnDataFiles(mydf5, fileName5, expectedSchema)

  val checksResult = Map(
    mydf1 -> validCheck1,
    mydf2 -> validCheck2,
    mydf3 -> validCheck3,
    mydf4 -> validCheck4,
    mydf5 -> validCheck5
  )
  val validDataFrames = checksResult.filter(x => x._2 == true).keys

  //

  println("MERGING ALL TRANSFORMED DATAFRAMES TOGETHER ")

  val dfSeq = validDataFrames.map(df => transformData(df))
  val mergeSeqDf = dfSeq.reduce(_ union _)
  mergeSeqDf.show()

  //

  println("WRITE TRANSFORMED DATA TO Azure Data Lake Storage Gen2 ")

  writeDataFrameToParquetFileInContainer(mergeSeqDf)

  //

  println("UPLOAD TRANSFORMED DATA TO A TABLE IN DATA WAREHOUSE (AZURE SYNAPSE) ")

  configSparkForSynapseConnection()
  uploadTransformedDataToSynapseTable(mergeSeqDf)

  //

  println("END MY JAR")

  //


  private def readDataFromContainer(spark: SparkSession, fileName: String): DataFrame = {
    println(s"read data from file name : $fileName")
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .parquet(s"/mnt/$containerName/$fileName")
    df.show()
    return df
  }


  private def checkMatchingOnColumnsNames(df: DataFrame, fileName: String, expectedSchema: StructType): Boolean = (df.columns.toSet == expectedSchema.fieldNames.toSet)


  private def checkDataTypesMatchingOnColumns(df: DataFrame, fileName: String): Boolean = (df.schema("registration_dttm").dataType.typeName == "timestamp" && df.schema("id").dataType.typeName == "integer" && df.schema("salary").dataType.typeName == "double" && df.schema("first_name").dataType.typeName == "string" && df.schema("last_name").dataType.typeName == "string" && df.schema("email").dataType.typeName == "string" && df.schema("gender").dataType.typeName == "string" && df.schema("ip_address").dataType.typeName == "string" && df.schema("cc").dataType.typeName == "string" && df.schema("country").dataType.typeName == "string" && df.schema("birthdate").dataType.typeName == "string" && df.schema("title").dataType.typeName == "string" && df.schema("comments").dataType.typeName == "string")


  private def checkValidationOnDataFiles(df: DataFrame, fileName: String, expectedSchema: StructType): Boolean = {
    val result = (checkMatchingOnColumnsNames(df, fileName, expectedSchema) && checkDataTypesMatchingOnColumns(df, fileName))
    if (result) {
      println(s"$fileName valid")
    } else {
      println(s"$fileName not valid")
    }
    return result
  }


  private def transformData(df: DataFrame): DataFrame = {
    val dfModified = df.withColumnRenamed("cc", "cc_mod").withMetadata("cc_mod", Metadata.fromJson("{\"tag\": \"this column has been modified\"}"))
    val dataSet = dfModified.distinct()
    val resultDataFrame = dataSet.toDF()
    println(s"transform data success")
    return resultDataFrame
  }


  private def writeDataFrameToParquetFileInContainer(df: DataFrame): Unit = {
    val containerName = "level3"
    val fileName = "transformed_user_data.parquet"
    val outputPath = "/FileStore/tables/output"
    df.write.mode("overwrite").parquet(outputPath)
    dbutils.fs.ls(outputPath)
    val filteredParquetFiles = dbutils.fs.ls(outputPath).filter(_.name.contains("parquet"))
    if (filteredParquetFiles.nonEmpty) {
      val resultParquetFile = filteredParquetFiles.head
      println(s"write transformed data to parquet file '$fileName' in container named '$containerName'  ")
      println(s"path :  ${resultParquetFile.path}  ")
      dbutils.fs.cp(resultParquetFile.path, s"abfss://$containerName@$storageAccountName.dfs.core.windows.net/$fileName")
    }
  }


  private def configSparkForSynapseConnection(): Unit = {
    println("CONFIG SPARK")
    println("Connect to Synapse using OAuth 2.0 with an Azure service principal")

    spark.conf.set(s"fs.azure.account.auth.type.$storageAccountName.dfs.core.windows.net", "OAuth")
    spark.conf.set(s"fs.azure.account.oauth.provider.type.$storageAccountName.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
    spark.conf.set(s"fs.azure.account.oauth2.client.id.$storageAccountName.dfs.core.windows.net", appId)
    spark.conf.set(s"fs.azure.account.oauth2.client.secret.$storageAccountName.dfs.core.windows.net", serviceCredential)
    spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.$storageAccountName.dfs.core.windows.net", s"https://login.microsoftonline.com/$tenantId/oauth2/token")
    spark.conf.set("spark.databricks.sqldw.jdbc.service.principal.client.id", appId)
    spark.conf.set("spark.databricks.sqldw.jdbc.service.principal.client.secret", serviceCredential)
    spark.conf.set(s"fs.azure.account.key.$storageAccountName.dfs.core.windows.net", storageAccountAccessKey)
  }


  private def uploadTransformedDataToSynapseTable(df: DataFrame): Unit = {
    val tableName = "People"
    val directoryName = "synapse_temp_data"
    val containerName = "container1"
    val url = s"jdbc:sqlserver://$server;database=$database;user=$username;password=$password;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30"
    df.write
      .format("com.databricks.spark.sqldw")
      .option("url", url)
      .option("forwardSparkAzureStorageCredentials", "true")
      .option("dbTable", tableName)
      .option("tempDir", s"abfss://$containerName@$storageAccountName.dfs.core.windows.net/$directoryName")
      .mode("overwrite")
      .save()
    println("uploaded data to synapse table success ")
  }


